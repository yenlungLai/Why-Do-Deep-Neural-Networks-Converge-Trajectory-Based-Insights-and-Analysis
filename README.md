# Wormhole Dynamics in Deep Neural Networks: Bridging Noise and Meaningful Labels



This work investigates the generalization behavior of deep neural networks (DNNs), with a focus on the phenomenon of 'fooling examples,' where DNNs confidently classify inputs that appear random or meaningless to humans. To explore this phenomenon, we introduce an analytical framework based on maximum likelihood estimation, which eliminates the need for gradient-based optimization or explicit labels. Our analysis reveals that DNNs operating in an overparameterized regime exhibit a collapse in the output feature space, enhancing feature generalization and improving the clustering of input data. This collapse is further supported by our newly derived 'wormhole' solution, which, when given an arbitrary fooling example, enables the reconciliation of a meaningful label with a random label and offers a novel perspective on shortcut learning. These insights deepen the understanding of DNN generalization, particularly in unsupervised settings, and suggest new directions for enhancing model robustness in practical applications.





# this work currently under review
